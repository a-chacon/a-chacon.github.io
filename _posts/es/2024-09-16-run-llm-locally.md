---
layout: post
title: "LLM en local: Corriendo Ollama y Open WebUI con Docker Compose."
category:
  - Docker
excerpt: "Buscando una alternativa segura, m√°s privada y con costo 0 a ChatGPT, llegu√© a Ollama y Open WebUI. Estas dos herramientas nos permiten correr un ChatGPT localmente."
image: /assets/images/ollama.webp
author: Andr√©s
comments: true
---

**Suponiendo que usas ChatGPT constantemente y que probablemente hayas o√≠do hablar de algunos de los √∫ltimos modelos ling√º√≠sticos de c√≥digo abierto (LLM), como Llama 3.1, Gemma 2 y Mistral, te voy a mostrar c√≥mo correr uno de estos en tu m√°quina local mediante Docker y Docker Compose.**

[Ollama](https://ollama.com/) se define como una **aplicaci√≥n de c√≥digo abierto** que permite ejecutar, crear y compartir localmente grandes modelos ling√º√≠sticos con una interfaz de l√≠nea de comandos en macOS y Linux. Y [Open WebUI](https://openwebui.com) es una **WebUI autoalojada extensible**, rica en caracter√≠sticas y f√°cil de usar, dise√±ada para operar completamente fuera de l√≠nea.

### Ventajas

Las ventajas que te puede traer tener tu LLM corriendo en local pueden ser:

- **Personalizaci√≥n**: Ejecutar los modelos localmente te ofrece un control total sobre el entorno. Puedes ajustar los modelos para adaptarlos a tus necesidades espec√≠ficas, ajustar los par√°metros e incluso experimentar con diferentes configuraciones.

- **Costes reducidos**: Si ya dispones de una m√°quina capaz, especialmente una equipada con una GPU, ejecutar LLMs localmente puede ser una opci√≥n rentable. No es necesario pagar por costosos recursos de computaci√≥n en la nube y puedes experimentar libremente sin preocuparte por los l√≠mites de llamadas a la API ni por el aumento de los costes.

- **Privacidad**: Cuando ejecutas modelos localmente, tus datos permanecen en tu m√°quina. Esto garantiza que la informaci√≥n confidencial nunca salga de su entorno seguro, proporcionando un nivel de privacidad que los servicios basados en la nube simplemente no pueden igualar. Para las empresas que manejan datos confidenciales, esto puede ser una ventaja crucial.

**Para m√≠, a nivel personal, me funciona bastante bien con un Ryzen 7 5600U.**

### ‚öôÔ∏è Manos a la obra

**Probablemente, si est√°s aqu√≠ ya conoces Docker y Docker Compose, as√≠ que me saltar√© cualquier introducci√≥n a estas herramientas.**

Crea una carpeta/directorio y luego agrega un archivo llamado `docker-compose.yml` donde agregar√°s el siguiente contenido:

```yml
services:
  webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - 3000:8080/tcp
    volumes:
      - open-webui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama
    expose:
      - 11434/tcp
    ports:
      - 11434:11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    volumes:
      - ollama:/root/.ollama

volumes:
  ollama:
  open-webui:
```

Ejecuta `docker compose up` y listo. Tendr√°s Ollama y Open WebUI corriendo.

#### √öltimo y m√°s importante paso‚ùó

Me pas√≥ que no todos los posts acerca de esto fueron claros con el tema, y por eso decid√≠ escribir esto en mi blog. Ollama es un 'gestor' de LLMs y, por defecto, no contiene ninguna imagen. Por lo que, cuando entres por primera vez a `localhost:3000`, ver√°s algo como esto:

![](/assets/images/openwebui.png)

Para solucionarlo, tienes que hacer pull de alguna imagen. Para esto **tienes dos opciones**: existe una forma gr√°fica de hacerlo desde Open WebUI y otra por l√≠nea de comandos directamente en el contenedor de Ollama. Esta segunda opci√≥n fue la que us√© yo; puedes hacerlo simplemente ejecutando la siguiente l√≠nea:

```bash
docker exec -it ollama-ollama-1 ollama pull llama3.1
```

Ten en cuenta que el nombre del contenedor depender√° de tus configuraciones; adem√°s, probablemente debas ejecutarlo usando `sudo`.

**[Aqu√≠](https://ollama.com/library) puedes ver todos los modelos disponibles.**

ü§ìAdem√°s, puedes interactuar con el modelo a nivel de l√≠nea de comandos. Si entras al contenedor de Ollama, puedes ejecutar `ollama run llama3.1` e interactuar sin la necesidad de una UI.ü§ì

¬°Hasta aqu√≠ por ahora! Disfruta de tu LLM local.
