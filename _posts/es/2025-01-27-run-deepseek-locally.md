---
layout: post
title: "LLM en local: Corriendo DeepSeek R1 y Open WebUI con Docker Compose."
category:
  - Docker
excerpt: "DeepSeek est√° irrumpiendo en la escena de la IA, un LLM de c√≥digo abierto y accesible que ha hecho caer los precios de las acciones de NVIDIA y otras empresas m√°s. Y como no, as√≠ como Ollama tambi√©n podremos correr este modelo de forma local para ver c√≥mo anda."
image: /assets/images/deepseek.jpg
author: Andr√©s
comments: true
---

**Suponiendo que usas ChatGPT constantemente y que probablemente hayas o√≠do hablar de algunos de los √∫ltimos modelos ling√º√≠sticos de c√≥digo abierto (LLM), como Llama 3.1, Gemma 2, Mistral o DeepSeek, te voy a mostrar c√≥mo correr este √∫ltimo en tu m√°quina local mediante Docker y Docker Compose.**

DeepSeek es un modelo ling√º√≠stico de inteligencia artificial de c√≥digo abierto desarrollado por una empresa china. Es una alternativa transparente y accesible a otros LLMs privados, permitiendo su uso y modificaci√≥n tanto para investigaci√≥n como para aplicaciones comerciales.

No lo niego, estoy disfrutando al ver c√≥mo se derrumba el monopolio de OpenAI y su LLM de c√≥digo cerrado frente a una alternativa de c√≥digo abierto y mucho m√°s econ√≥mica, tanto en precio como en consumo de recursos.

[Ollama](https://ollama.com/) se define como una **aplicaci√≥n de c√≥digo abierto** que permite ejecutar, crear y compartir localmente grandes modelos ling√º√≠sticos con una interfaz de l√≠nea de comandos en macOS y Linux. Y [Open WebUI](https://openwebui.com) es una **WebUI autoalojada extensible**, rica en caracter√≠sticas y f√°cil de usar, dise√±ada para operar completamente fuera de l√≠nea.

### Ventajas

Las ventajas que te puede traer tener tu LLM corriendo en local pueden ser:

- **Personalizaci√≥n**: Ejecutar los modelos localmente te ofrece un control total sobre el entorno. Puedes ajustar los modelos para adaptarlos a tus necesidades espec√≠ficas, ajustar los par√°metros e incluso experimentar con diferentes configuraciones.

- **Costes reducidos**: Si ya dispones de una m√°quina capaz, especialmente una equipada con una GPU, ejecutar LLMs localmente puede ser una opci√≥n rentable. No es necesario pagar por costosos recursos de computaci√≥n en la nube y puedes experimentar libremente sin preocuparte por los l√≠mites de llamadas a la API ni por el aumento de los costes.

- **Privacidad**: Cuando ejecutas modelos localmente, tus datos permanecen en tu m√°quina. Esto garantiza que la informaci√≥n confidencial nunca salga de su entorno seguro, proporcionando un nivel de privacidad que los servicios basados en la nube simplemente no pueden igualar. Para las empresas que manejan datos confidenciales, esto puede ser una ventaja crucial.

**Para m√≠, a nivel personal, me funciona bastante bien con un Ryzen 7 5600U.**

### ‚öôÔ∏è Manos a la obra

**Probablemente, si est√°s aqu√≠ ya conoces Docker y Docker Compose, as√≠ que me saltar√© cualquier introducci√≥n a estas herramientas.**

Crea una carpeta/directorio y luego agrega un archivo llamado `docker-compose.yml` donde agregar√°s el siguiente contenido:

```yml
services:
  webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - 3000:8080/tcp
    volumes:
      - open-webui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama
    expose:
      - 11434/tcp
    ports:
      - 11434:11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    volumes:
      - ollama:/root/.ollama

volumes:
  ollama:
  open-webui:
```

Ejecuta `docker compose up` y listo. Tendr√°s Ollama y Open WebUI corriendo.

#### √öltimo y m√°s importante paso‚ùó

Me pas√≥ que no todos los posts acerca de esto fueron claros con el tema, y por eso decid√≠ escribir esto en mi blog. Ollama es un 'gestor' de LLMs y, por defecto, no contiene ninguna imagen. Por lo que, cuando entres por primera vez a `localhost:3000`, ver√°s algo como esto:

![Open Web Ui empty without models](/assets/images/openwebui.png)

Para solucionarlo, tienes que hacer pull de alguna imagen. Para esto **tienes dos opciones**: existe una forma gr√°fica de hacerlo desde Open WebUI y otra por l√≠nea de comandos directamente en el contenedor de Ollama. Esta segunda opci√≥n fue la que us√© yo; puedes hacerlo simplemente ejecutando la siguiente l√≠nea:

```bash
docker exec -it ollama-ollama-1 ollama pull deepseek-r1
```

Ten en cuenta que el nombre del contenedor depender√° de tus configuraciones; adem√°s, probablemente debas ejecutarlo usando `sudo`.

**[Aqu√≠](https://ollama.com/library) puedes ver todos los modelos disponibles.**

ü§ìAdem√°s, puedes interactuar con el modelo a nivel de l√≠nea de comandos. Si entras al contenedor de Ollama, puedes ejecutar `ollama run deepseek-r1` e interactuar sin la necesidad de una UI.ü§ì

¬°Hasta aqu√≠ por ahora! Disfruta de tu LLM local.

## Docker Compose Files For development

  Acabo de publicar un repositorio incluyendo este archivo de docker compose (archivos que uso a diario para trabajar). Puedes clonarlo y levantar todos los contenedores que necesites para desarrollar tus aplicaciones. Tambi√©n est√° abierto a contribuciones:

<https://codeberg.org/a-chacon/docker-compose-for-development>
